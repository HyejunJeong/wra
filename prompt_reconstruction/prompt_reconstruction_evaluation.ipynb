{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Reconstruction Evaluation Notebook\n",
    "\n",
    "This notebook provides a comprehensive evaluation framework for prompt reconstruction tasks, integrating multiple evaluation metrics and utilities from the `prompt_reconstruction/src` codebase.\n",
    "\n",
    "## Features:\n",
    "- SBERT-based semantic similarity evaluation\n",
    "- LLM judge-based evaluation\n",
    "- OBELS (multi-dimensional behavioral similarity) evaluation\n",
    "- Utility measurement for research reports\n",
    "- Batch processing of evaluation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "from types import SimpleNamespace\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from pathlib import Path\n",
    "\n",
    "# Enable nested event loops for Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Add the src directory to Python path\n",
    "src_path = Path('./src')\n",
    "if src_path.exists():\n",
    "    sys.path.append(str(src_path))\n",
    "    print(f\"‚úÖ Added {src_path.resolve()} to Python path\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Warning: src directory not found. Make sure to run this notebook from the prompt_reconstruction directory.\")\n",
    "    print(f\"Current working directory: {Path.cwd()}\")\n",
    "    print(\"Expected structure: prompt_reconstruction/src/\")\n",
    "    # Try alternative paths\n",
    "    alt_paths = ['prompt_reconstruction/src', '../src']\n",
    "    for alt_path in alt_paths:\n",
    "        if Path(alt_path).exists():\n",
    "            sys.path.append(str(Path(alt_path).resolve()))\n",
    "            print(f\"‚úÖ Found and added alternative path: {Path(alt_path).resolve()}\")\n",
    "            break\n",
    "\n",
    "# Import evaluation utilities\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from utils.utils_model import get_llm_model\n",
    "    from utils.utils_eval import (\n",
    "        compute_sbert_similarity, \n",
    "        Compute_LLM_Judge_Similarity,\n",
    "        Handle_OBELS_Scores,\n",
    "        HandleWholeFolder,\n",
    "        process_single_file,\n",
    "        print_and_save_results\n",
    "    )\n",
    "    from measure_utility import (\n",
    "        custom_report_example,\n",
    "        load_llm_model,\n",
    "        calculate_utility_score\n",
    "    )\n",
    "    print(\"‚úÖ Successfully imported evaluation modules\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Make sure you're running this notebook from the correct directory and have installed all dependencies.\")\n",
    "    print(\"Required packages: sentence-transformers, openai, pandas, numpy\")\n",
    "    print(\"\\nInstall missing packages with:\")\n",
    "    print(\"pip install sentence-transformers openai pandas numpy matplotlib seaborn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configuration Setup\n",
    "\n",
    "# Configuration parameters\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # API Keys - Set these as environment variables for security\n",
    "        self.openai_api_key = os.getenv('OPENAI_API_KEY', '')\n",
    "        self.tavily_api_key = os.getenv('TAVILY_API_KEY', '')\n",
    "        \n",
    "        # Model settings\n",
    "        self.model = \"gpt-4o-mini\"\n",
    "        self.temperature = 0.3\n",
    "        self.max_tokens = 1000\n",
    "        self.llm_sleep = 1.0  # Delay between LLM calls to avoid rate limiting\n",
    "        \n",
    "        # SBERT model settings\n",
    "        self.sbert_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        self.cache_folder = None  # Will use default cache\n",
    "        \n",
    "        # File paths (update these according to your data)\n",
    "        self.input_folder = \"../data/reconstructed_prompts/\"  # Folder containing CSV files to evaluate\n",
    "        self.output_dir = \"../results/evaluation_results/\"  # Output directory for results\n",
    "        \n",
    "        # Processing settings\n",
    "        self.start_index = 0  # Index to start processing from\n",
    "\n",
    "    def validate_setup(self):\n",
    "        \"\"\"Validate configuration and provide helpful feedback\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        if not self.openai_api_key:\n",
    "            issues.append(\"‚ö†Ô∏è  OPENAI_API_KEY not set. LLM-based evaluations will not work.\")\n",
    "        \n",
    "        if not self.tavily_api_key:\n",
    "            issues.append(\"‚ö†Ô∏è  TAVILY_API_KEY not set. Utility evaluation will not work.\")\n",
    "        \n",
    "        if not Path(self.input_folder).exists():\n",
    "            issues.append(f\"‚ö†Ô∏è  Input folder not found: {self.input_folder}\")\n",
    "        \n",
    "        if issues:\n",
    "            print(\"Configuration Issues:\")\n",
    "            for issue in issues:\n",
    "                print(f\"  {issue}\")\n",
    "            print(\"\\nTo fix API key issues, set environment variables:\")\n",
    "            print(\"  export OPENAI_API_KEY='your_openai_key'\")\n",
    "            print(\"  export TAVILY_API_KEY='your_tavily_key'\")\n",
    "            print(\"\\nOr set them in Python:\")\n",
    "            print(\"  os.environ['OPENAI_API_KEY'] = 'your_key'\")\n",
    "        else:\n",
    "            print(\"‚úÖ Configuration validated successfully\")\n",
    "        \n",
    "        return len(issues) == 0\n",
    "        \n",
    "# Create configuration instance\n",
    "config = Config()\n",
    "print(f\"Model: {config.model}\")\n",
    "print(f\"SBERT Model: {config.sbert_model}\")\n",
    "config.validate_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set API Keys (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set API Keys (if needed)\n",
    "\n",
    "# Option 1: Set via environment variables (recommended)\n",
    "print(\"üîë Setting up API keys...\")\n",
    "print(\"Option 1 (Recommended): Set as environment variables\")\n",
    "print(\"  export OPENAI_API_KEY='your_openai_key'\")\n",
    "print(\"  export TAVILY_API_KEY='your_tavily_key'\")\n",
    "\n",
    "print(\"\\nOption 2: Set in notebook (less secure)\")\n",
    "print(\"Uncomment and fill in your API keys below:\")\n",
    "\n",
    "# SECURITY WARNING: Never commit notebooks with real API keys to version control!\n",
    "# os.environ['OPENAI_API_KEY'] = 'your_openai_api_key_here'\n",
    "# os.environ['TAVILY_API_KEY'] = 'your_tavily_api_key_here'\n",
    "\n",
    "# Update config with API keys after setting them\n",
    "config.openai_api_key = os.getenv('OPENAI_API_KEY', '')\n",
    "config.tavily_api_key = os.getenv('TAVILY_API_KEY', '')\n",
    "\n",
    "print(f\"\\nAPI Key Status:\")\n",
    "print(f\"  OpenAI API Key: {'‚úÖ Set' if config.openai_api_key else '‚ùå Not set'}\")\n",
    "print(f\"  Tavily API Key: {'‚úÖ Set' if config.tavily_api_key else '‚ùå Not set'}\")\n",
    "\n",
    "if not config.openai_api_key:\n",
    "    print(\"\\nüí° To set OpenAI API key in this session:\")\n",
    "    print(\"   os.environ['OPENAI_API_KEY'] = 'your-key-here'\")\n",
    "    \n",
    "if not config.tavily_api_key:\n",
    "    print(\"\\nüí° To set Tavily API key in this session:\")\n",
    "    print(\"   os.environ['TAVILY_API_KEY'] = 'your-key-here'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize Models\n",
    "\n",
    "# Convert config to args-like object for compatibility\n",
    "args = SimpleNamespace(**config.__dict__)\n",
    "\n",
    "# Initialize models\n",
    "llm_model = None\n",
    "sbert_model = None\n",
    "\n",
    "print(\"üîÑ Initializing models...\")\n",
    "\n",
    "try:\n",
    "    # Initialize SBERT model first (this always works)\n",
    "    print(\"  Loading SBERT model...\")\n",
    "    sbert_model = SentenceTransformer(config.sbert_model, cache_folder=config.cache_folder)\n",
    "    print(\"  ‚úÖ SBERT model loaded successfully\")\n",
    "    \n",
    "    # Initialize LLM model (requires API key)\n",
    "    if config.openai_api_key:\n",
    "        print(\"  Loading LLM model...\")\n",
    "        llm_model = get_llm_model(args)\n",
    "        print(\"  ‚úÖ LLM model loaded successfully\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è  LLM model not loaded (OpenAI API key required)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå Error loading models: {e}\")\n",
    "    print(f\"     Error type: {type(e).__name__}\")\n",
    "    \n",
    "    # Try to give helpful error messages\n",
    "    if \"sentence-transformers\" in str(e).lower():\n",
    "        print(\"     Try installing: pip install sentence-transformers\")\n",
    "    elif \"openai\" in str(e).lower():\n",
    "        print(\"     Try installing: pip install openai\")\n",
    "\n",
    "# Model status summary\n",
    "print(\"\\nüìã Model Status:\")\n",
    "print(f\"  SBERT Model: {'‚úÖ Ready' if sbert_model else '‚ùå Failed'}\")\n",
    "print(f\"  LLM Model: {'‚úÖ Ready' if llm_model else '‚ùå Not available'}\")\n",
    "\n",
    "# Set environment variable for OpenAI if model loaded successfully\n",
    "if llm_model and config.openai_api_key:\n",
    "    os.environ['OPENAI_API_KEY'] = config.openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single File Evaluation\n",
    "\n",
    "Evaluate a single CSV file containing original and reconstructed prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_file(file_path, llm_model, sbert_model, args, save_results=True):\n",
    "    \"\"\"\n",
    "    Evaluate a single CSV file with prompt reconstruction results.\n",
    "    \n",
    "    Expected CSV format:\n",
    "    - 'prompts' or 'Unnamed: 0': Original prompts\n",
    "    - 'reconstructed_prompt': Reconstructed prompts\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"‚ùå File not found: {file_path}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        print(f\"üìä Evaluating file: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        # Process the file using the evaluation utilities\n",
    "        df = process_single_file(file_path, llm_model, sbert_model, args)\n",
    "        \n",
    "        if save_results:\n",
    "            # Save results\n",
    "            result = print_and_save_results(df, args, os.path.basename(file_path), os.path.dirname(file_path))\n",
    "            return df, result\n",
    "        else:\n",
    "            return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error evaluating file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage - update the file path to your actual data\n",
    "sample_file = \"../data/sample_reconstructed_prompts.csv\"  # Update this path\n",
    "\n",
    "# Uncomment to run evaluation on a single file\n",
    "# if os.path.exists(sample_file):\n",
    "#     result = evaluate_single_file(sample_file, llm_model, sbert_model, args)\n",
    "#     if result:\n",
    "#         df, summary = result\n",
    "#         print(\"\\nüìà Evaluation Summary:\")\n",
    "#         print(f\"Average SBERT similarity: {summary['avg_sbert_similarity']:.4f}\")\n",
    "#         print(f\"Average LLM judge similarity: {summary['avg_llm_judge_similarity']:.4f}\")\n",
    "# else:\n",
    "#     print(f\"Sample file not found: {sample_file}\")\n",
    "#     print(\"Please update the file path to point to your actual data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Evaluation\n",
    "\n",
    "Process multiple CSV files in a folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_evaluation(input_folder, llm_model, sbert_model, args):\n",
    "    \"\"\"\n",
    "    Run evaluation on all CSV files in the specified folder.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"‚ùå Input folder not found: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        print(f\"üîÑ Starting batch evaluation of folder: {input_folder}\")\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "        \n",
    "        # Run batch evaluation\n",
    "        HandleWholeFolder(input_folder, llm_model, sbert_model, args)\n",
    "        \n",
    "        print(f\"‚úÖ Batch evaluation completed. Results saved to: {args.output_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in batch evaluation: {e}\")\n",
    "\n",
    "# Example usage\n",
    "# Uncomment to run batch evaluation\n",
    "# run_batch_evaluation(config.input_folder, llm_model, sbert_model, args)\n",
    "\n",
    "print(f\"To run batch evaluation, uncomment the line above and update the input folder path:\")\n",
    "print(f\"Current input folder: {config.input_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Evaluation\n",
    "\n",
    "Evaluate the utility of research reports generated from different domain visibility levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_utility(csv_file_path, llm_model, args):\n",
    "    \"\"\"\n",
    "    Evaluate utility of research reports at different domain visibility levels.\n",
    "    \n",
    "    Expected CSV format:\n",
    "    - 'prompts': Research queries\n",
    "    - 'domain_sequence': Original domain sequence\n",
    "    - 'domain_0.2_visibility', 'domain_0.4_visibility', etc.: Domain sequences at different visibility levels\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        print(f\"‚ùå File not found: {csv_file_path}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        print(f\"üîç Starting utility evaluation for: {os.path.basename(csv_file_path)}\")\n",
    "        \n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        print(f\"Loaded {len(df)} rows\")\n",
    "        \n",
    "        # Process only a subset for demo (can be adjusted)\n",
    "        start_idx = getattr(args, 'start_index', 0)\n",
    "        end_idx = min(start_idx + 5, len(df))  # Process only 5 samples for demo\n",
    "        \n",
    "        for index in range(start_idx, end_idx):\n",
    "            row = df.iloc[index]\n",
    "            print(f\"\\nProcessing row {index}: {row['prompts'][:50]}...\")\n",
    "            \n",
    "            query = row[\"prompts\"]\n",
    "            \n",
    "            # Generate and evaluate original report\n",
    "            if 'domain_sequence' in row and pd.notna(row['domain_sequence']):\n",
    "                org_report = await custom_report_example(query, row[\"domain_sequence\"].split(\",\"))\n",
    "                df.loc[index, 'org_report'] = org_report\n",
    "                \n",
    "                utility_score = calculate_utility_score(llm_model, query, org_report, args.max_tokens)\n",
    "                df.loc[index, 'org_utility_score'] = utility_score\n",
    "                print(f\"Original utility score: {utility_score}\")\n",
    "            \n",
    "            # Evaluate different visibility levels\n",
    "            visibility_levels = [\"domain_0.2_visibility\", \"domain_0.4_visibility\", \n",
    "                               \"domain_0.6_visibility\", \"domain_0.8_visibility\"]\n",
    "            \n",
    "            for domain_list_name in visibility_levels:\n",
    "                if domain_list_name in row and pd.notna(row[domain_list_name]):\n",
    "                    domain_list = row[domain_list_name].split(\",\")\n",
    "                    new_report = await custom_report_example(query, domain_list)\n",
    "                    df.loc[index, domain_list_name + \"_report\"] = new_report\n",
    "                    \n",
    "                    utility_score = calculate_utility_score(llm_model, query, new_report, args.max_tokens)\n",
    "                    df.loc[index, domain_list_name + \"_utility_score\"] = utility_score\n",
    "                    print(f\"{domain_list_name} utility score: {utility_score}\")\n",
    "        \n",
    "        # Save results\n",
    "        output_path = os.path.join(args.output_dir, f\"utility_results_{os.path.basename(csv_file_path)}\")\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"‚úÖ Utility evaluation completed. Results saved to: {output_path}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in utility evaluation: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "utility_file = \"../data/sample_utility_data.csv\"  # Update this path\n",
    "\n",
    "# Uncomment to run utility evaluation\n",
    "# if os.path.exists(utility_file) and config.tavily_api_key and llm_model:\n",
    "#     result = await evaluate_utility(utility_file, llm_model, args)\n",
    "# else:\n",
    "#     print(\"Utility evaluation requires:\")\n",
    "#     print(f\"- Valid data file (currently: {utility_file})\")\n",
    "#     print(f\"- Tavily API key: {'‚úÖ' if config.tavily_api_key else '‚ùå'}\")\n",
    "#     print(f\"- LLM model: {'‚úÖ' if llm_model else '‚ùå'}\")\n",
    "\n",
    "print(\"To run utility evaluation, uncomment the code above and update the file path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Evaluation Example\n",
    "\n",
    "Create and evaluate a small sample dataset for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for demonstration\n",
    "sample_data = {\n",
    "    'prompts': [\n",
    "        \"Find information about climate change impacts on agriculture\",\n",
    "        \"Research the best programming languages for machine learning\",\n",
    "        \"Look up recent developments in quantum computing\"\n",
    "    ],\n",
    "    'reconstructed_prompt': [\n",
    "        \"Search for climate change effects on farming and crops\",\n",
    "        \"Find top programming languages for AI and ML development\",\n",
    "        \"Research latest quantum computing breakthroughs and advances\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "sample_df = pd.DataFrame(sample_data)\n",
    "print(\"üìã Sample dataset created:\")\n",
    "print(sample_df)\n",
    "\n",
    "# Evaluate the sample data\n",
    "if sbert_model:\n",
    "    print(\"\\nüîç Computing SBERT similarities...\")\n",
    "    sample_df['sbert_similarity'] = compute_sbert_similarity(sample_df, sbert_model)\n",
    "    \n",
    "    print(\"\\nüìä SBERT Similarity Results:\")\n",
    "    for idx, row in sample_df.iterrows():\n",
    "        print(f\"Row {idx+1}: {row['sbert_similarity']:.4f}\")\n",
    "    \n",
    "    avg_similarity = sample_df['sbert_similarity'].mean()\n",
    "    print(f\"\\nAverage SBERT similarity: {avg_similarity:.4f}\")\n",
    "\n",
    "# LLM Judge evaluation (if LLM model is available)\n",
    "if llm_model and config.openai_api_key:\n",
    "    print(\"\\nü§ñ Computing LLM Judge similarities...\")\n",
    "    try:\n",
    "        sample_df['llm_judge_similarity'] = Compute_LLM_Judge_Similarity(sample_df, llm_model, args)\n",
    "        \n",
    "        print(\"\\nüìä LLM Judge Similarity Results:\")\n",
    "        for idx, row in sample_df.iterrows():\n",
    "            print(f\"Row {idx+1}: {row['llm_judge_similarity']:.4f}\")\n",
    "        \n",
    "        avg_llm_similarity = sample_df['llm_judge_similarity'].mean()\n",
    "        print(f\"\\nAverage LLM Judge similarity: {avg_llm_similarity:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing LLM Judge similarities: {e}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  LLM Judge evaluation skipped (requires OpenAI API key and LLM model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Results Analysis and Visualization\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    print(\"‚úÖ Visualization libraries imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import visualization libraries: {e}\")\n",
    "    print(\"Install with: pip install matplotlib seaborn\")\n",
    "\n",
    "def analyze_results(df):\n",
    "    \"\"\"\n",
    "    Analyze and visualize evaluation results.\n",
    "    \"\"\"\n",
    "    print(\"üìà Analysis of Evaluation Results\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"‚ö†Ô∏è  No data to analyze\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìä Dataset info: {len(df)} samples\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    metric_columns = ['sbert_similarity', 'llm_judge_similarity']\n",
    "    available_metrics = [col for col in metric_columns if col in df.columns]\n",
    "    \n",
    "    if not available_metrics:\n",
    "        print(\"‚ö†Ô∏è  No similarity metrics found in data\")\n",
    "        return\n",
    "    \n",
    "    for metric in available_metrics:\n",
    "        stats = df[metric].describe()\n",
    "        print(f\"\\n{metric.replace('_', ' ').title()} Statistics:\")\n",
    "        print(f\"  Count: {stats['count']:.0f}\")\n",
    "        print(f\"  Mean:  {stats['mean']:.4f}\")\n",
    "        print(f\"  Std:   {stats['std']:.4f}\")\n",
    "        print(f\"  Min:   {stats['min']:.4f}\")\n",
    "        print(f\"  Max:   {stats['max']:.4f}\")\n",
    "    \n",
    "    # Visualization (only if matplotlib is available)\n",
    "    try:\n",
    "        fig, axes = plt.subplots(1, len(available_metrics), figsize=(6*len(available_metrics), 5))\n",
    "        if len(available_metrics) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        colors = ['blue', 'green', 'red', 'orange']\n",
    "        \n",
    "        for i, metric in enumerate(available_metrics):\n",
    "            axes[i].hist(df[metric], bins=min(15, len(df)//2), alpha=0.7, color=colors[i])\n",
    "            axes[i].set_title(f'{metric.replace(\"_\", \" \").title()} Distribution')\n",
    "            axes[i].set_xlabel('Similarity Score')\n",
    "            axes[i].set_ylabel('Frequency')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add mean line\n",
    "            mean_val = df[metric].mean()\n",
    "            axes[i].axvline(mean_val, color='red', linestyle='--', \n",
    "                          label=f'Mean: {mean_val:.3f}', alpha=0.8)\n",
    "            axes[i].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Correlation analysis if multiple metrics available\n",
    "        if len(available_metrics) >= 2:\n",
    "            correlation = df[available_metrics[0]].corr(df[available_metrics[1]])\n",
    "            print(f\"\\nüîó Correlation between {available_metrics[0]} and {available_metrics[1]}: {correlation:.4f}\")\n",
    "            \n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.scatter(df[available_metrics[0]], df[available_metrics[1]], alpha=0.7)\n",
    "            plt.xlabel(available_metrics[0].replace('_', ' ').title())\n",
    "            plt.ylabel(available_metrics[1].replace('_', ' ').title())\n",
    "            plt.title(f'Correlation Analysis (r={correlation:.3f})')\n",
    "            plt.plot([0, 1], [0, 1], 'r--', alpha=0.5, label='Perfect correlation')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Visualization failed: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Demo analysis with sample data (if no real results available yet)\n",
    "if 'sample_df' in locals() and 'sbert_similarity' in sample_df.columns:\n",
    "    print(\"üìä Analyzing sample results...\")\n",
    "    analyze_results(sample_df)\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Run evaluation cells above to generate results for analysis\")\n",
    "    print(\"   This function will be available once you have evaluation results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Evaluation Notebook Summary\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"\\n‚úÖ Available Evaluation Methods:\")\n",
    "print(\"1. SBERT-based semantic similarity\")\n",
    "print(\"2. LLM judge-based similarity evaluation\")\n",
    "print(\"3. OBELS multi-dimensional behavioral similarity\")\n",
    "print(\"4. Utility evaluation for research reports\")\n",
    "\n",
    "print(\"\\nüìÅ Key Functions:\")\n",
    "print(\"- evaluate_single_file(): Evaluate one CSV file\")\n",
    "print(\"- run_batch_evaluation(): Process multiple files in a folder\")\n",
    "print(\"- evaluate_utility(): Measure research report utility\")\n",
    "print(\"- analyze_results(): Analyze and visualize results\")\n",
    "\n",
    "print(\"\\nüîß Configuration:\")\n",
    "print(f\"- Model: {config.model}\")\n",
    "print(f\"- SBERT Model: {config.sbert_model}\")\n",
    "print(f\"- Output Directory: {config.output_dir}\")\n",
    "print(f\"- OpenAI API: {'‚úÖ' if config.openai_api_key else '‚ùå Not configured'}\")\n",
    "print(f\"- Tavily API: {'‚úÖ' if config.tavily_api_key else '‚ùå Not configured'}\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"1. Set your API keys in the configuration section\")\n",
    "print(\"2. Update file paths to point to your actual data\")\n",
    "print(\"3. Uncomment and run the evaluation functions\")\n",
    "print(\"4. Analyze results using the provided visualization tools\")\n",
    "\n",
    "print(\"\\nüìä Expected Data Format:\")\n",
    "print(\"CSV files should contain:\")\n",
    "print(\"- 'prompts' or 'Unnamed: 0': Original prompts\")\n",
    "print(\"- 'reconstructed_prompt': Reconstructed prompts\")\n",
    "print(\"- For utility evaluation: domain visibility columns\")\n",
    "\n",
    "print(\"\\nüí° Tips:\")\n",
    "print(\"- Use environment variables for API keys for security\")\n",
    "print(\"- Start with small datasets to test the pipeline\")\n",
    "print(\"- Monitor API usage to avoid rate limits\")\n",
    "print(\"- Save intermediate results frequently\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
