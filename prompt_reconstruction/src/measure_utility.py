import argparse
import os
from openai import OpenAI
import numpy as np
import pandas as pd
import json
import asyncio
import nest_asyncio  # Required for notebooks/interactive environments
from gpt_researcher import GPTResearcher

# Apply nest_asyncio to handle nested event loops
nest_asyncio.apply()

async def custom_report_example(query, domain_list):
    """Demonstrate various custom prompt examples with GPT Researcher."""
    
    # Define your research query
    report_type = "research_report"
    
    # Initialize the researcher
    researcher = GPTResearcher(
        query=query,
        report_type=report_type,
        report_source="web",
        query_domains=domain_list,
        #verbose=True  # Set to True to see detailed logs
    )
    
    # Conduct the research (this step is the same regardless of custom prompts)
    print("üîç Conducting research...")
    research_result = await researcher.conduct_research()
    print("‚úÖ Research completed!\n")
    
    standard_report = await researcher.write_report()
    
    return standard_report

def load_llm_model(args):
    client = OpenAI(api_key=args.openai_api_key)
    # Store model and temperature for later use
    client.model = args.model
    client.temperature = args.temperature
    return client

def get_llm_response(llm_model, prompt, max_tokens):
    response = llm_model.chat.completions.create(
        model=llm_model.model,
        messages=[{"role": "user", "content": prompt}],
        temperature=llm_model.temperature,
        max_tokens=max_tokens
    )
    return response.choices[0].message.content

def calculate_utility_score(llm_model, query, report, max_tokens):
    prompt = f"""
    You are an impartial evaluator tasked with assessing the **utility** of a research report generated by an autonomous web research agent.

    ## Definition of Utility
    Utility means how helpful the report is for a human reader to achieve their research goals.  
    Key aspects of utility include:
    - **Coverage**: How well does the report cover relevant aspects of the research question?
    - **Depth**: How detailed and well-explained are the findings?
    - **Accuracy & Evidence**: Are claims supported with sources, citations, or reasoning?
    - **Clarity**: Is the report clearly structured and easy to understand?
    - **Actionability**: Does the report provide insights or conclusions that a user could act upon?

    ## Instructions
    1. Read the research question and the report.  
    2. Evaluate the report's utility using the dimensions above.  
    3. Assign a **utility score** between 1 and 10.  
    - `1` = very low utility, incomplete, or misleading.  
    - `10` = extremely useful, comprehensive, reliable.  
    4. Provide your output strictly in **valid JSON**.

    ## Input
    Research Question: {query}  
    Report: {report}

    ## Output Format
    {{
    "research_question": "...",
    "coverage_score": 0,
    "depth_score": 0,
    "accuracy_score": 0,
    "clarity_score": 0,
    "actionability_score": 0,
    "overall_utility_score": 0,
    "justification": "..."
    }}


    Note: make sure to use "[]" instead of "()" to have valid JSON. Your output should be a valid JSON object.
    """
    # calculate the utility score
    output_text = get_llm_response(llm_model, prompt, max_tokens)
    scores = json.loads(output_text)
    return scores["overall_utility_score"]

async def main(args, llm_model):
    print("Starting main function...")
    df = pd.read_csv(args.file_path)
    print(f"Loaded CSV with {len(df)} rows")
    
    for index, row in df.iterrows():
        print(f"Processing row {index}")
        if index < 10:
            print(f"Skipping row {index}")
            continue
        print(f"Processing row {index}: {row['prompts'][:50]}...")
        
        query = row["prompts"]
        org_report = await custom_report_example(query, row["domain_sequence"].split(","))
        df.loc[index, 'org_report'] = org_report
        utility_score = calculate_utility_score(llm_model, query, org_report, args.max_tokens)
        df.loc[index, 'org_utility_score'] = utility_score
        
        # calculate the utility score for the new report
        domain_list_names = ["domain_0.2_visibility", "domain_0.4_visibility", "domain_0.6_visibility", "domain_0.8_visibility"]
        for domain_list_name in domain_list_names:
            domain_list = row[domain_list_name].split(",")
            new_report = await custom_report_example(query, domain_list)
            df.loc[index, domain_list_name + "_report"] = new_report
            utility_score = calculate_utility_score(llm_model, query, new_report, args.max_tokens)
            df.loc[index, domain_list_name + "_utility_score"] = utility_score
    
    print("Saving results to CSV...")
    # save the df to a csv file
    df.to_csv(args.output_file_path, index=False)
    print("Main function completed successfully!")

if __name__ == "__main__":
    args = argparse.ArgumentParser()
    args.add_argument("--tavily_api_key", type=str, default="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")
    args.add_argument("--file_path", type=str, default="data/GPT-Researcher-Utility/Utility_Evaluation Set - SESSION14-GR-GPT4.csv")
    args.add_argument("--output_file_path", type=str, default="data/GPT-Researcher-Utility/Utility_report.csv")
    args.add_argument("--openai_api_key", type=str, default="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")
    args.add_argument("--model", type=str, default="gpt-4o-mini")
    args.add_argument("--temperature", type=float, default=0.3)
    args.add_argument("--max_tokens", type=int, default=1000)
    
    args = args.parse_args()
    os.environ["OPENAI_API_KEY"] = args.openai_api_key
    os.environ["TAVILY_API_KEY"] = args.tavily_api_key
    llm_model = load_llm_model(args)
    asyncio.run(main(args, llm_model))