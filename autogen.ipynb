{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a63585e8-f26c-4b2b-9e3d-e2d8284711b0",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9252c198-2a6a-42d5-bc7a-528d4ceb50d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your_openai_key_here\"\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"your_gemini_key_here\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"your_google_key_here\"\n",
    "os.environ[\"DEEPSEEK_API_KEY\"] = \"your_deepseek_key_here\"\n",
    "\n",
    "\n",
    "import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST.json\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\", \"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "llm_config = {\n",
    "    \"timeout\": 60,\n",
    "    \"cache_seed\": 42,\n",
    "    \"config_list\": config_list,\n",
    "    \"temperature\": 0,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9403aee7-b7a4-47b5-a95f-8d2a77e673ba",
   "metadata": {},
   "source": [
    "# Autogen Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65efaa4-bffc-42f1-8ba8-6199788f0fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "\n",
    "def parse_topic_file(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    topics = re.findall(r\"<topic num='(\\d+)'>\\s*<desc>(.*?)</desc>\", content, re.DOTALL)\n",
    "    return [(int(num), desc.strip()) for num, desc in topics]\n",
    "\n",
    "def get_public_ip():\n",
    "    try:\n",
    "        return urllib.request.urlopen(\"https://api.ipify.org\").read().decode(\"utf-8\")\n",
    "    except Exception:\n",
    "        return \"unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70316714-b84b-4d66-b85f-f4eaaba2a8ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "import logging\n",
    "import datetime\n",
    "import socket\n",
    "import time\n",
    "import subprocess\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_ext.models.ollama import OllamaChatCompletionClient\n",
    "from autogen_ext.agents.web_surfer import MultimodalWebSurfer\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "isp_logger = logging.getLogger(\"isp_logger\")\n",
    "isp_logger.setLevel(logging.INFO)\n",
    "isp_logger.propagate = False\n",
    "\n",
    "\n",
    "def set_isp_logfile(filename):\n",
    "    for h in isp_logger.handlers:\n",
    "        isp_logger.removeHandler(h)\n",
    "    handler = logging.FileHandler(filename, mode=\"w\")\n",
    "    handler.setFormatter(logging.Formatter(\"%(levelname)s | %(message)s\"))\n",
    "    isp_logger.addHandler(handler)\n",
    "\n",
    "def start_tcpdump(topic_num):\n",
    "    return subprocess.Popen([\n",
    "        \"sudo\", \"tcpdump\", \"-i\", \"any\",\n",
    "        \"-w\", f\"autogen_log/full_traffic_{topic_num}.pcap\"\n",
    "    ])\n",
    "    \n",
    "\n",
    "class LoggingOpenAIClient(OpenAIChatCompletionClient):\n",
    "    async def acompletion(self, *args, **kwargs):\n",
    "        messages = kwargs.get(\"messages\") or (args[0] if args else [])\n",
    "        timestamp = datetime.datetime.utcnow().isoformat()\n",
    "        domain = \"api.openai.com\"\n",
    "        ip = socket.gethostbyname(domain)\n",
    "        method = \"POST\"\n",
    "        try:\n",
    "            payload_size = len(str(messages).encode())\n",
    "        except Exception:\n",
    "            payload_size = \"unknown\"\n",
    "\n",
    "        isp_logger.info(f\"[ISP] {timestamp} | [API REQUEST] Method: {method} | Domain: {domain} | IP: {ip} | URL: https://api.openai.com/v1/chat/completions | Size: {payload_size}\")\n",
    "        return await super().acompletion(*args, **kwargs)\n",
    "\n",
    "\n",
    "class LoggingWebSurfer(MultimodalWebSurfer):\n",
    "\n",
    "    async def _lazy_init(self):\n",
    "        await super()._lazy_init()\n",
    "\n",
    "        original_visit_page = self._playwright_controller.visit_page\n",
    "        original_on_new_page = self._playwright_controller.on_new_page\n",
    "\n",
    "        async def attach_logging_to_page(page):\n",
    "            async def log_request(route, request):\n",
    "                url = request.url\n",
    "                domain = urlparse(url).netloc or \"unknown\"\n",
    "                timestamp = datetime.datetime.utcnow().isoformat()\n",
    "                try:\n",
    "                    ip = socket.gethostbyname(domain)\n",
    "                except Exception:\n",
    "                    ip = \"unknown\"\n",
    "                method = request.method\n",
    "                size = request.headers.get(\"content-length\", \"unknown\")\n",
    "                \n",
    "                # if \"bing.com\" not in domain:\n",
    "                isp_logger.info(f\"[ISP] {timestamp} | Domain: {domain} | IP: {ip} | Size: {size} | [REQUEST] Method: {method} |  URL: {url} \")\n",
    "\n",
    "                await route.continue_()\n",
    "    \n",
    "            def log_response(response):\n",
    "                async def handle():\n",
    "                    timestamp = datetime.datetime.utcnow().isoformat()\n",
    "                    try:\n",
    "                        body = await response.body()\n",
    "                        size = len(body)\n",
    "                    except Exception:\n",
    "                        size = \"unknown\"\n",
    "                    status = response.status\n",
    "                    resp_url = response.url\n",
    "                    domain = urlparse(resp_url).netloc or \"unknown\"\n",
    "                    try:\n",
    "                        ip = socket.gethostbyname(domain)\n",
    "                    except Exception:\n",
    "                        ip = \"unknown\"\n",
    "\n",
    "                    # if \"bing.com\" not in domain:\n",
    "                    isp_logger.info(f\"[ISP] {timestamp} | Domain: {domain} | IP: {ip} | Size: {size} | [RESPONSE] Status: {status} |  URL: {resp_url} \")\n",
    "                asyncio.create_task(handle())\n",
    "    \n",
    "            await page.route(\"**/*\", log_request)\n",
    "            page.on(\"response\", log_response)\n",
    "\n",
    "            page.on(\"framenavigated\", lambda frame: isp_logger.info(\n",
    "                f\"[ISP] {datetime.datetime.utcnow().isoformat()} | [NAVIGATED] | Domain: {urlparse(frame.url).netloc} | URL: {frame.url}\"\n",
    "            ))\n",
    "\n",
    "\n",
    "        async def logging_visit_page(page, url, *args, **kwargs):\n",
    "            await attach_logging_to_page(page)\n",
    "            page.context.on(\"page\", lambda p: asyncio.create_task(attach_logging_to_page(p)))\n",
    "            return await original_visit_page(page, url, *args, **kwargs)\n",
    "    \n",
    "        async def logging_on_new_page(page):\n",
    "            await original_on_new_page(page)\n",
    "            await attach_logging_to_page(page)\n",
    "    \n",
    "        self._playwright_controller.visit_page = logging_visit_page\n",
    "        self._playwright_controller.on_new_page = logging_on_new_page\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2decf182-daf9-4eba-934a-d34e4e6ea5da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from autogen import GroupChat\n",
    "\n",
    "async def run_all_tasks():\n",
    "\n",
    "    \n",
    "    log_file = f\"autogen_log/rewritten/gpt4o-DD16-1.txt\"\n",
    "\n",
    "    logger = set_isp_logfile(log_file)\n",
    "\n",
    "    assistant = AssistantAgent(\n",
    "        name=\"LLM_Assistant\",\n",
    "        system_message=(\n",
    "            \"You are part of a web browsing team.\\n\"\n",
    "            \"Your role is to collaborate with the MultimodalWebSurfer.\\n\"\n",
    "            \"You must not simulate browsing or guess what might be on websites.\\n\"\n",
    "            \"Instead, wait for the MultimodalWebSurfer to visit web pages and return content before you respond.\\n\"\n",
    "            \"Only use information the surfer provides. Do not use prior knowledge or make assumptions.\\n\"\n",
    "            \"Summarize findings only after the surfer has visited at least 5 pages.\\n\"\n",
    "            \"Conclude with 'Done with task.' once your summary is complete.\"\n",
    "        ),\n",
    "\n",
    "        model_client=LoggingOpenAIClient(model=\"gpt-4o\")\n",
    "        # model_client=LoggingOpenAIClient(model=\"gemini-2.0-flash\")\n",
    "        # model_client=OllamaChatCompletionClient(model=\"llama3:8b\")\n",
    "    )\n",
    "\n",
    "    web_surfer_agent = LoggingWebSurfer(\n",
    "        name=\"MultimodalWebSurfer\",\n",
    "        model_client=OpenAIChatCompletionClient(model=\"gpt-4o-2024-08-06\"),\n",
    "        # model_client=OpenAIChatCompletionClient(model=\"gemini-2.0-flash\"),\n",
    "        # model_client=OllamaChatCompletionClient(model=\"llama3:8b\")\n",
    "    )\n",
    "\n",
    "    termination = MaxMessageTermination(100) | TextMentionTermination(\"Done with task.\", sources=\"MultimodalWebSurfer\") \\\n",
    "                                        | TextMentionTermination(\"ou're welcome\") \\\n",
    "                                        | TextMentionTermination(\"here's anything else\") \\\n",
    "                                        | TextMentionTermination(\"need more assistance\") \n",
    "\n",
    "    agent_team = RoundRobinGroupChat(\n",
    "        [assistant, web_surfer_agent],\n",
    "        termination_condition=termination\n",
    "    )\n",
    "    \n",
    "    full_prompt = f\"\"\"**Instructions for Researcher**\n",
    "\n",
    "**Research Task:**  \n",
    "I am seeking a detailed report on the U.S. military’s deployment to West Africa to support the response to the Ebola epidemic, with a focus on the following aspects:\n",
    "\n",
    "1. **Key Leaders:**  \n",
    "   - Identify field-grade officers (majors, lieutenant colonels, colonels, or equivalent ranks) and senior NCOs from all four U.S. military branches (Army, Navy, Air Force, Marine Corps) who were in direct command of units deployed to West Africa for Ebola support operations.  \n",
    "   - For each leader, provide:\n",
    "     - Name\n",
    "     - Rank and Service\n",
    "     - Specific position/title during the deployment\n",
    "     - Unit commanded or managed\n",
    "     - Location of deployment (country/site if possible)\n",
    "   - Exclude U.S. military leaders who were not deployed to Africa and any non-U.S. or civilian aid worker leadership.\n",
    "   \n",
    "2. **Mission Description:**  \n",
    "   - Describe the official mission of the deployed units.  \n",
    "   - Clarify the scope of activities (e.g., facilities construction, logistics, telecommunications, road building, etc.), with an explicit emphasis that military personnel were not involved in direct patient care or treatment of Ebola victims.\n",
    "   - Quote or summarize official mission statements or orders where available.\n",
    "   \n",
    "3. **Personnel Safety Protocols:**  \n",
    "   - Document the official protocols and procedures enforced to safeguard deployed U.S. personnel against Ebola exposure.\n",
    "   - Include pre-deployment, in-theater, and post-deployment protocols, specifically including mandatory quarantine or monitoring upon return to home bases.\n",
    "   - Note any service-specific variations in safety procedures.\n",
    "   - Reference official military or Defense Department sources describing these protocols.\n",
    "   \n",
    "4. **Scope:**  \n",
    "   - Only include information about U.S. military units that deployed to Africa for the Ebola epidemic response.\n",
    "   - Exclude non-U.S. military and civilian organization details.\n",
    "   - Exclude U.S. military units that did not physically deploy to Africa.\n",
    "   \n",
    "5. **Table Requirement:**  \n",
    "   - Present a table summarizing the key leaders, ranks, branches, units, deployment locations, and roles.\n",
    "   \n",
    "6. **Expected Output Format:**  \n",
    "   - Provide a structured report including the following sections and headers:\n",
    "     1. Executive Summary\n",
    "     2. Key Leaders Table\n",
    "     3. Mission Overview\n",
    "     4. Safety Protocols for Personnel\n",
    "     5. References\n",
    "     \n",
    "7. **Preferred Sources:**  \n",
    "   - Use official or primary sources where possible, such as:\n",
    "     - U.S. Department of Defense press releases and reports\n",
    "     - Official statements from U.S. Africa Command, U.S. Army Africa, and other service branches\n",
    "     - Public documentation from service component commands (e.g., www.defense.gov, www.army.mil, www.africom.mil)\n",
    "   - Avoid sources focused on non-U.S. units or civilian aid efforts.\n",
    "   - Reference only materials in English unless otherwise instructed.\n",
    "   \n",
    "8. **Open-Ended Attributes:**  \n",
    "   - If the precise names/titles of some field-grade officers or senior NCOs cannot be identified in open sources, note this explicitly and provide the highest available command-level detail.\n",
    "   \n",
    "**Please ensure the report is well-organized, directly responsive to the parameters above, and cites all sources used. Tables, headers, and structured format are required.**\n",
    "\"\"\"\n",
    "# To complete the task, you must:\n",
    "# 1. Begin with a Bing search.\n",
    "# 2. Click links or manually visit at least 5 different pages—not just summarize search results or rely on prior knowledge.\n",
    "# 3. Visit and extract information from at least 5 different websites by clicking links.\n",
    "# 4. Only use information found on pages you actually opened.\n",
    "# 5. Once you have gathered concrete details from at least 5 different pages, write a brief summary and say: \"Done with task.\"\n",
    "\n",
    "# Do not infer or assume anything unless it is directly stated on a visited webpage.\n",
    "# \"\"\"\n",
    "\n",
    "    stream = agent_team.run_stream(task=full_prompt)\n",
    "    await Console(stream, output_stats=True)\n",
    "    await web_surfer_agent.close()\n",
    "\n",
    "    # tcpdump_proc.terminate()\n",
    "    # tcpdump_proc.wait()\n",
    "\n",
    "    for handler in isp_logger.handlers:\n",
    "        handler.flush()\n",
    "    time.sleep(1)\n",
    "\n",
    "\n",
    "# Run the batch\n",
    "asyncio.run(run_all_tasks())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88778a7-9795-4a84-88df-bed71a3e621b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from autogen import GroupChat\n",
    "\n",
    "# def parse_topic_file(filename):\n",
    "#     with open(filename, \"r\") as f:\n",
    "#         lines = [line.strip() for line in f if line.strip()]\n",
    "#     return [(i + 1, line) for i, line in enumerate(lines)]\n",
    "\n",
    "async def run_all_tasks():\n",
    "    topics = parse_topic_file(\"topictext-890.txt\")\n",
    "    # topics = parse_topic_file(\"topictext-5prompt-autogen.txt\")\n",
    "\n",
    "    for topic_num, task_desc in topics:\n",
    "        log_file = f\"autogen_log/gemini-2/agent_traffic_{topic_num}.txt\"\n",
    "\n",
    "        if topic_num not in [2, 4, 22, 29, 34, 36, 41, 42, 51, 57]:\n",
    "            continue\n",
    "        if os.path.exists(log_file) and os.path.getsize(log_file) > 0:\n",
    "            print(f\"Skipping topic {topic_num}: already completed.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Running topic {topic_num}\")\n",
    "        \n",
    "        # log_file = f\"rerun/autogen_agent_traffic_{topic_num}.txt\"\n",
    "        logger = set_isp_logfile(log_file)\n",
    "\n",
    "        assistant = AssistantAgent(\n",
    "            name=\"LLM_Assistant\",\n",
    "            system_message=(\n",
    "                \"You are part of a web browsing team.\\n\"\n",
    "                \"Your role is to collaborate with the MultimodalWebSurfer.\\n\"\n",
    "                \"You must not simulate browsing or guess what might be on websites.\\n\"\n",
    "                \"Instead, wait for the MultimodalWebSurfer to visit web pages and return content before you respond.\\n\"\n",
    "                \"Only use information the surfer provides. Do not use prior knowledge or make assumptions.\\n\"\n",
    "                \"Summarize findings only after the surfer has visited at least 5 pages.\\n\"\n",
    "                \"Conclude with 'Done with task.' once your summary is complete.\"\n",
    "            ),\n",
    "\n",
    "            # model_client=LoggingOpenAIClient(model=\"gpt-4o\")\n",
    "            model_client=LoggingOpenAIClient(model=\"gemini-2.0-flash\")\n",
    "            # model_client=OllamaChatCompletionClient(model=\"llama3:8b\")\n",
    "        )\n",
    "\n",
    "        web_surfer_agent = LoggingWebSurfer(\n",
    "            name=\"MultimodalWebSurfer\",\n",
    "            # model_client=OpenAIChatCompletionClient(model=\"gpt-4o-2024-08-06\"),\n",
    "            model_client=OpenAIChatCompletionClient(model=\"gemini-2.0-flash\"),\n",
    "            # model_client=OllamaChatCompletionClient(model=\"llama3:8b\")\n",
    "        )\n",
    "    \n",
    "        termination = MaxMessageTermination(100) | TextMentionTermination(\"Done with task.\", sources=\"MultimodalWebSurfer\") \\\n",
    "                                            | TextMentionTermination(\"ou're welcome\") \\\n",
    "                                            | TextMentionTermination(\"here's anything else\") \\\n",
    "                                            | TextMentionTermination(\"need more assistance\") \n",
    "\n",
    "        agent_team = RoundRobinGroupChat(\n",
    "            [assistant, web_surfer_agent],\n",
    "            termination_condition=termination\n",
    "        )\n",
    "        \n",
    "        full_prompt = f\"\"\"{task_desc.strip()}\n",
    "\n",
    "Before concluding or summarizing, please collaborate with the MultimodalWebSurfer to explore relevant websites and gather concrete information.\n",
    "\n",
    "To complete the task, you must:\n",
    "1. Begin with a Bing search.\n",
    "2. Click links or manually visit at least 5 different pages—not just summarize search results or rely on prior knowledge.\n",
    "3. Visit and extract information from at least 5 different websites by clicking links.\n",
    "4. Only use information found on pages you actually opened.\n",
    "5. Once you have gathered concrete details from at least 5 different pages, write a brief summary and say: \"Done with task.\"\n",
    "\n",
    "Do not infer or assume anything unless it is directly stated on a visited webpage.\n",
    "\"\"\"\n",
    "\n",
    "        stream = agent_team.run_stream(task=full_prompt)\n",
    "        await Console(stream, output_stats=True)\n",
    "        await web_surfer_agent.close()\n",
    "\n",
    "        for handler in isp_logger.handlers:\n",
    "            handler.flush()\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "# Run the batch\n",
    "asyncio.run(run_all_tasks())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5a92f3-6ada-41f1-b31d-2ddf410a13f6",
   "metadata": {},
   "source": [
    "# Filter traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ab03b8-9313-457b-9d63-3061de348f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Directory and file config\n",
    "# log_dir = \"autogen_log\"\n",
    "log_dir = \"rerun\"\n",
    "log_prefix = \"autogen_agent_traffic_\"\n",
    "log_suffix = \".txt\"\n",
    "# num_files = 60\n",
    "num_files = 20\n",
    "\n",
    "# Keywords that indicate noisy, non-user-initiated domains\n",
    "NOISY_DOMAIN_KEYWORDS = [\n",
    "    \"doubleclick\", \"hubspot\", \"adspsp\", \"onetrust.\", \"googlesyndication\", \"adsrvr\", \"adnxs\", \"adsafeprotected\", \"pubmatic\", \"rubiconproject\", \"msn\", \"bing\", \"googlevideo.\", \"googletagmanager.\", \"adform\", \"salesforceliveagent\",\n",
    "    \"mdn\", \"admaster\", \"criteo\", \"rtb\", \"yieldmo\", \"taboola\", \"outbrain\", \"quantserve\", \"insightexpress\",\n",
    "    \"adservice\", \"amazon-adsystem\", \"aps.amazon\", \"adtrafficquality.google\", \"ads.\", \"advertising\",\n",
    "    \"banner\", \"cookie\", \"ezodn\", \"ezoic\", \"adlightning\", \"liadm.com\", \"media.net\", \"pm-serv.co\",\n",
    "    \"rqtrk.eu\", \"mgid\", \"adblocker\", \"admanmedia\", \"hotjar.\", \"clickguard\", \"contextweb\", \"doubleverify\",\n",
    "    \"analytics\", \"google-analytics\", \"facebook\", \"twitter\", \"linkedin\", \"scorecardresearch\", \"pixel\",\n",
    "    \"tracking\", \"track\", \"metrics.\", \"sentry\", \"clarity.ms\", \"fullstory\", \"crazyegg\", \"segment\",\n",
    "    \"cdn\", \"fonts\", \"gstatic\", \"cloudfront\", \"cloudflare\", \"moatads\", \"optimizely\", \"js.\", \"api.\",\n",
    "    \"virtualearth\", \"apis.\", \"wp.com\", \"hlx.page\", \"vo.msecnd.net\", \"mpio.io\",\n",
    "    \"amazonaws.com\", \"objectstorage\", \"azureedge.net\", \"fastly.net\", \"cdn.jsdelivr.net\", \"akamaihd.net\",\n",
    "    \"qualified\", \"recombee\", \"leadsrx\", \"crwdcntrl\", \"typekit.\", \n",
    "    \"imasdk\", \"translate.google\", \"calendar.google\", \"accounts.google\", \"googleapis\", \"cse.google.com\",\n",
    "    \"fundingchoicesmessages.google.com\", \"google.\", \"bidsxchange\", \"avplayer\", \"usercontent.\", \"css.\", \"scripts.\",\n",
    "    \"privacymanager.io\", \"foresee.com\", \"qualaroo\", \"trustarc\", \"truste\", \"gatekeeperconsent\",\n",
    "    \"givebutter\", \"churnkey\", \"fundraiseup\", \"mailerlite\", \"marketo\", \"pardot\", \"hsforms\", \"paperform\",\n",
    "    \"activedemand\", \"activehosted\", \"convertkit\", \"script.\", \"googletag\", \"widget\",\n",
    "    \"qualtrics\", \"monsido\", \"igodigital\", \"drivetheweb\", \"addtoany\", \"liveperson.\", \n",
    "    \"squarespace\", \"cloudinary\", \"wistia\", \"forbesimg\", \"imageio.forbes.com\", \"ytimg\", \"open.video\",\n",
    "    \"humix\", \"recommendation.forbes.com\", \"analytics.\", \"google.\", \"tiktok\", \"server.\", \n",
    "    \"amazonaws.\", \"api-domain-compado.com\", \"xapstream\", \"adroll\", \"-app\", \"3lift.\", \"beacon\",\n",
    "    \"pinterest\", \"pinimg.com\", \"quora\", \"instagram\", \"yimg.com\", \"recaptcha\", \"sharethis\", \"highperformanceformat.com\", \"fontawesome.\", \".content\", \"ad.gt\", \"assets-\", \n",
    "    \"jwplayer\", \"assets.\", \"jquery.\", \"static.\", \"asset-\", \"app-\", \"app.\", \"js-\", \"widget.\", \"captcha\", \"permutive.app\", \"youtube.\", \"lytics\", \"adthrive.\", \"wikimedia\", \"doubleverify\",\n",
    "]\n",
    "\n",
    "def is_noisy(domain):\n",
    "    domain = domain.lower()\n",
    "    return any(keyword in domain for keyword in NOISY_DOMAIN_KEYWORDS)\n",
    "\n",
    "def extract_size(line):\n",
    "    match = re.search(r\"Size:\\s*(\\d+|unknown)\", line)\n",
    "    if match:\n",
    "        size_str = match.group(1)\n",
    "        return int(size_str) if size_str.isdigit() else None\n",
    "    return None\n",
    "\n",
    "\n",
    "# Output dictionary\n",
    "filtered_domain_seqs = {}\n",
    "\n",
    "for i in range(1, num_files + 1):\n",
    "    filename = f\"{log_prefix}{i}{log_suffix}\"\n",
    "    path = os.path.join(log_dir, filename)\n",
    "\n",
    "    valid_domains = []\n",
    "    seen = set()\n",
    "\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if \"[ISP]\" in line and \"Domain:\" in line:\n",
    "                size = extract_size(line)\n",
    "                if size is None or size < 7000:\n",
    "                    continue  # skip if size unknown or too small\n",
    "\n",
    "                domain_match = re.search(r\"Domain:\\s*([\\w\\.-]+)\", line)\n",
    "                if domain_match:\n",
    "                    domain = domain_match.group(1).lower()\n",
    "                    if not is_noisy(domain) and domain not in seen:\n",
    "                        valid_domains.append(domain)\n",
    "                        seen.add(domain)\n",
    "\n",
    "    filtered_domain_seqs[f\"prompt_{i}\"] = valid_domains\n",
    "\n",
    "# Save output\n",
    "with open(\"rerun/autogen_filtered_domain_sequences.json\", \"w\") as f:\n",
    "    json.dump(filtered_domain_seqs, f, indent=2)\n",
    "\n",
    "print(\"✅ Finished: filtered domain sequences saved to autogen_filtered_domain_sequences.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c6a113-3b52-4c3c-ae83-bed155e3ca78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5d8283-0fff-440f-8e51-5e71d2fa3ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\", \n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"web\",\n",
    "        \"use_docker\": False,\n",
    "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"Reply TERMINATE if the task has been solved at full satisfaction.\n",
    "Otherwise, reply CONTINUE, or the reason why the task is not solved yet.\"\"\",\n",
    ")\n",
    "\n",
    "from datetime import datetime\n",
    "from autogen.tools.experimental import DuckDuckGoSearchTool \n",
    "\n",
    "import requests\n",
    "\n",
    "class DuckDuckGoWithUserAgent(DuckDuckGoSearchTool):\n",
    "    def _get_html(self, url):\n",
    "        headers = {\n",
    "            \"User-Agent\": (\n",
    "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                \"Chrome/117.0.0.0 Safari/537.36\"\n",
    "            )\n",
    "        }\n",
    "        return requests.get(url, headers=headers, timeout=10)\n",
    "\n",
    "wrapped_tool = ToolWrapperWithISPLogger(\n",
    "    DuckDuckGoWithUserAgent(),\n",
    "    base_url=\"https://html.duckduckgo.com/html/\"\n",
    ")\n",
    "\n",
    "wrapped_tool.register_for_llm(assistant)\n",
    "wrapped_tool.register_for_execution(user_proxy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0e7064-52a0-4c77-b4f6-4077974466cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an AssistantAgent instance named \"assistant\"\n",
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "# create a UserProxyAgent instance named \"user_proxy\"\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"TERMINATE\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"web\",\n",
    "        \"use_docker\": False,\n",
    "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"Reply TERMINATE if the task has been solved at full satisfaction.\n",
    "Otherwise, reply CONTINUE, or the reason why the task is not solved yet.\"\"\",\n",
    ")\n",
    "user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message=\"\"\"Compare recent advancements in LLM alignment techniques across OpenAI, Anthropic, and Meta. Provide key papers, blog posts, and implementations.\"\"\",\n",
    ")\n",
    "\n",
    "for msg in chat_result.chat_history:\n",
    "    print(f\"{msg['role']} ({msg.get('name', 'n/a')}):\\n{msg.get('content', '[no content]')}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7324dbc4-c9a1-4f50-959e-3f115b49e616",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AutoGen)",
   "language": "python",
   "name": "autogen-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
