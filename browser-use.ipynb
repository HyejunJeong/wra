{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8fce621-f60d-4477-ac8b-1307738e7e7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Browser Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dce1b0-2513-4650-8546-50b2bb9fba17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your_openai_key_here\"\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"your_gemini_key_here\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"your_google_key_here\"\n",
    "os.environ[\"DEEPSEEK_API_KEY\"] = \"your_deepseek_key_here\"\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "from browser_use import Agent, BrowserSession\n",
    "from pydantic import SecretStr\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "# ISP-style logger (used per topic)\n",
    "def setup_isp_logger(path):\n",
    "    logger = logging.getLogger(\"isp_logger\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.propagate = False\n",
    "    for h in logger.handlers:\n",
    "        logger.removeHandler(h)\n",
    "    handler = logging.FileHandler(path, mode=\"w\")\n",
    "    handler.setFormatter(logging.Formatter(\"%(levelname)s | %(message)s\"))\n",
    "    logger.addHandler(handler)\n",
    "    return logger\n",
    "\n",
    "# # Load prompts from XML/text file\n",
    "# def parse_topic_file(path):\n",
    "#     import re\n",
    "#     with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         content = f.read()\n",
    "#     topics = re.findall(r\"<topic num='(\\d+)'>\\s*<desc>(.*?)</desc>\", content, re.DOTALL)\n",
    "#     return [(int(num), desc.strip()) for num, desc in topics]\n",
    "\n",
    "def parse_topic_file(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "    return [(i + 1, line) for i, line in enumerate(lines)]\n",
    "\n",
    "async def run_all_browser_use_topics():\n",
    "\n",
    "    isp_log_path = f\"browseruse_log/rewritten/gpt4o-inst-SESSION14-1.txt\"        \n",
    "    isp_logger = setup_isp_logger(isp_log_path)\n",
    "\n",
    "    browser_session = BrowserSession(user_data_dir=None, headless=True)\n",
    "    await browser_session.start()\n",
    "\n",
    "    full_prompt = f\"\"\"Instructions for Researcher\n",
    "        \n",
    "Objective:  \n",
    "I want to learn about traditional Swahili dishes and how to cook them, so I can surprise my friend from Kenya. Please find relevant web pages that focus specifically on Swahili home cooking.\n",
    "Instructions:\n",
    "\n",
    "1. **Focus on Swahili Cuisine**  \n",
    "   - Prioritize dishes that are traditional to the Swahili-speaking coastal regions of Kenya and East Africa.\n",
    "   - Ensure the dishes you find are genuinely Swahili rather than general Kenyan or East African food unless they are widely recognized as part of Swahili home cuisine.\n",
    "   \n",
    "2. **Type of Resources**  \n",
    "   - Seek out web pages that provide recipes, step-by-step instructions, or cooking guides, ideally from home cooks, food bloggers, or reputable culinary sites.\n",
    "   - Prioritize resources that suit home cooking rather than professional/restaurant preparation.\n",
    "   \n",
    "3. **Format of Output**  \n",
    "   - Structure your findings as a short report with clear headers:\n",
    "     1. **Overview of Swahili Cuisine**  \n",
    "     2. **List of Popular Swahili Dishes**  \n",
    "     3. **Cooking Instructions/Resources**  \n",
    "     4. **References and Links**  \n",
    "   - Under each dish, include a brief description and the direct web page link to the cooking instructions.\n",
    "   \n",
    "4. **Web Page Selection**  \n",
    "   - Prefer web pages with detailed and easy-to-follow instructions, pictures, or videos.\n",
    "   - Give special consideration to resources published by Kenyans, Swahili communities, or sites focusing on authentic regional cuisine.\n",
    "   \n",
    "5. **Presentation**  \n",
    "   - Include a table listing at least 5 traditional Swahili dishes, with columns for: Dish Name, Brief Description, and Link to Cooking Resource.\n",
    "   - Indicate if any dishes have special significance (e.g., eaten at celebrations or daily meals).\n",
    "   \n",
    "6. **Open-Ended Preferences**  \n",
    "   - No specific ingredient restrictions or dietary requirements were provided, so include all common Swahili dishes.\n",
    "   - No language requirement is stated; prioritize English language resources, but include notable Swahili-language resources if especially authentic or instructive.\n",
    "   \n",
    "7. **Sources**  \n",
    "   - Prefer official or reputable food sites, blogs by authentic home cooks, YouTube channels from Kenyan or Swahili creators, and cultural organizations.\n",
    "   - Avoid aggregator or SEO-heavy copycat blogs.\n",
    "   \n",
    "8. **References and Linking**  \n",
    "   - For each dish/resource, provide a direct link to the web page, not just the homepage.\n",
    "   \n",
    "Expected Output:  \n",
    "A clearly formatted report with the above structure and a table cataloguing at least 5 Swahili dishes and direct links to recipes, followed by a references section with all sources linked.\n",
    "If you need information about a particular dish’s popularity or special significance and can’t determine it from the recipe page, make a note of that and suggest further research.\n",
    "\n",
    "Before concluding or summarizing, please collaborate with the MultimodalWebSurfer to explore relevant websites and gather concrete information. Use Bing as a search engine.\n",
    "\n",
    "To complete the task, you must:\n",
    "1. Click and visit pages—not just summarize search results or rely on prior knowledge.\n",
    "2. Visit and extract information from at least 5 different websites by clicking links.\n",
    "3. Only use information found on pages you actually opened.\n",
    "4. Once you have gathered concrete details from at least 5 pages, write a brief summary and say: \"Done with task.\"\n",
    "5. If you encounter reCAPTCHA, switch to Bing search. \n",
    "\n",
    "Do not infer or assume anything unless it is directly stated on a visited webpage.\n",
    "\"\"\"\n",
    "    \n",
    "    agent = Agent(\n",
    "        enable_memory=False,\n",
    "        task=full_prompt,\n",
    "        initial_actions=[{\"open_tab\": {\"url\": \"https://www.bing.com\"}}],\n",
    "        llm=ChatOpenAI(model=\"gpt-4o\"),\n",
    "        # llm = ChatGoogleGenerativeAI(model='gemini-2.0-flash', api_key=api_key),\n",
    "        # llm=ChatDeepSeek(base_url='https://api.deepseek.com/v1', model='deepseek-reasoner', api_key=SecretStr(api_key))\n",
    "\n",
    "        browser_session=browser_session,\n",
    "    )\n",
    "    \n",
    "    history = await agent.run()\n",
    "\n",
    "    # Simulate ISP logs\n",
    "    for url in history.urls():\n",
    "        domain = urlparse(url).netloc or \"unknown\"\n",
    "        timestamp = datetime.datetime.utcnow().isoformat()\n",
    "        try:\n",
    "            ip = socket.gethostbyname(domain)\n",
    "        except Exception:\n",
    "            ip = \"unknown\"\n",
    "        if \"bing.com\" not in domain:\n",
    "            isp_logger.info(\n",
    "                f\"[ISP] {timestamp} | Domain: {domain} | SrcIP: local | DstIP: {ip} | Size: unknown | [NAVIGATED] URL: {url}\"\n",
    "            )\n",
    "\n",
    "    await browser_session.close()\n",
    "\n",
    "# Run the full batch\n",
    "await run_all_browser_use_topics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e3a4ee-8ab6-4df5-b146-07dc5d2498c4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your_openai_key_here\"\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"your_gemini_key_here\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"your_google_key_here\"\n",
    "os.environ[\"DEEPSEEK_API_KEY\"] = \"your_deepseek_key_here\"\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "from browser_use import Agent, BrowserSession\n",
    "from pydantic import SecretStr\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "# ISP-style logger (used per topic)\n",
    "def setup_isp_logger(path):\n",
    "    logger = logging.getLogger(\"isp_logger\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.propagate = False\n",
    "    for h in logger.handlers:\n",
    "        logger.removeHandler(h)\n",
    "    handler = logging.FileHandler(path, mode=\"w\")\n",
    "    handler.setFormatter(logging.Formatter(\"%(levelname)s | %(message)s\"))\n",
    "    logger.addHandler(handler)\n",
    "    return logger\n",
    "\n",
    "# # Load prompts from XML/text file\n",
    "# def parse_topic_file(path):\n",
    "#     import re\n",
    "#     with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         content = f.read()\n",
    "#     topics = re.findall(r\"<topic num='(\\d+)'>\\s*<desc>(.*?)</desc>\", content, re.DOTALL)\n",
    "#     return [(int(num), desc.strip()) for num, desc in topics]\n",
    "\n",
    "def parse_topic_file(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "    return [(i + 1, line) for i, line in enumerate(lines)]\n",
    "\n",
    "async def run_all_browser_use_topics():\n",
    "    # topics = parse_topic_file(\"topictext-890.txt\")\n",
    "    topics = parse_topic_file(\"topictext-5prompt.txt\")\n",
    "\n",
    "    for topic_num, desc in topics:\n",
    "\n",
    "        isp_log_path = f\"browseruse_log/gemini-2.0/agent_traffic_{topic_num}.txt\"\n",
    "        \n",
    "        if os.path.exists(isp_log_path):\n",
    "            print(f\"Skipping topic {topic_num} (already completed).\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nRunning topic {topic_num}...\")\n",
    "        \n",
    "        isp_logger = setup_isp_logger(isp_log_path)\n",
    "\n",
    "        browser_session = BrowserSession(user_data_dir=None, headless=True)\n",
    "        await browser_session.start()\n",
    "\n",
    "        full_prompt = f\"\"\"{desc}\n",
    "\n",
    "Before concluding or summarizing, please collaborate with the MultimodalWebSurfer to explore relevant websites and gather concrete information. Use Bing as a search engine.\n",
    "\n",
    "To complete the task, you must:\n",
    "1. Click and visit pages—not just summarize search results or rely on prior knowledge.\n",
    "2. Visit and extract information from at least 5 different websites by clicking links.\n",
    "3. Only use information found on pages you actually opened.\n",
    "4. Once you have gathered concrete details from at least 5 pages, write a brief summary and say: \"Done with task.\"\n",
    "5. If you encounter reCAPTCHA, switch to Bing search. \n",
    "\n",
    "Do not infer or assume anything unless it is directly stated on a visited webpage.\n",
    "\"\"\"\n",
    "        # llm = ChatGoogle(model='gemini-2.0-flash-exp', api_key=api_key)\n",
    "\n",
    "        # api_key = os.getenv('GOOGLE_API_KEY')\n",
    "        \n",
    "        agent = Agent(\n",
    "            enable_memory=False,\n",
    "            task=full_prompt,\n",
    "            initial_actions=[{\"open_tab\": {\"url\": \"https://www.bing.com\"}}],\n",
    "            llm=ChatOpenAI(model=\"gpt-4o\"),\n",
    "            # llm = ChatGoogleGenerativeAI(model='gemini-2.0-flash', api_key=api_key),\n",
    "            # llm=ChatDeepSeek(base_url='https://api.deepseek.com/v1', model='deepseek-reasoner', api_key=SecretStr(api_key))\n",
    "\n",
    "            browser_session=browser_session,\n",
    "        )\n",
    "        \n",
    "        history = await agent.run()\n",
    "\n",
    "        # Simulate ISP logs\n",
    "        for url in history.urls():\n",
    "            domain = urlparse(url).netloc or \"unknown\"\n",
    "            timestamp = datetime.datetime.utcnow().isoformat()\n",
    "            try:\n",
    "                ip = socket.gethostbyname(domain)\n",
    "            except Exception:\n",
    "                ip = \"unknown\"\n",
    "            if \"bing.com\" not in domain:\n",
    "                isp_logger.info(\n",
    "                    f\"[ISP] {timestamp} | Domain: {domain} | SrcIP: local | DstIP: {ip} | Size: unknown | [NAVIGATED] URL: {url}\"\n",
    "                )\n",
    "\n",
    "        await browser_session.close()\n",
    "\n",
    "# Run the full batch\n",
    "await run_all_browser_use_topics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b101e6-b091-4f03-89db-9c69869b0583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "log_dir = \"browseruse_log\"\n",
    "log_prefix = \"agent_traffic_\"\n",
    "log_suffix = \".txt\"\n",
    "num_files = 60\n",
    "\n",
    "# Output dictionary: {filename: [domain1, domain2, ...]}\n",
    "all_domain_seqs = {}\n",
    "\n",
    "for i in range(1, num_files + 1):\n",
    "    filename = f\"{log_prefix}{i}{log_suffix}\"\n",
    "    path = os.path.join(log_dir, filename)\n",
    "\n",
    "    domain_seq = []\n",
    "    seen = set()\n",
    "\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if \"[ISP]\" in line and \"Domain:\" in line:\n",
    "                # Extract domain from the line\n",
    "                match = re.search(r\"Domain:\\s*([\\w\\.-]+)\", line)\n",
    "                if match:\n",
    "                    domain = match.group(1)\n",
    "                    if domain not in seen:\n",
    "                        domain_seq.append(domain)\n",
    "                        seen.add(domain)\n",
    "\n",
    "    all_domain_seqs[f\"prompt_{i}\"] = domain_seq\n",
    "\n",
    "# Optional: save as JSONL or print result\n",
    "import json\n",
    "with open(\"browseruse_domain_sequences.json\", \"w\") as f:\n",
    "    json.dump(all_domain_seqs, f, indent=2)\n",
    "\n",
    "print(\"✅ Extracted domain sequences from all sessions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ba6b78-c4e2-4748-b1f6-f8dc9c3466dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7759eec-1215-441d-aa98-e5510b671bec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AutoGen)",
   "language": "python",
   "name": "autogen-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
